{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Knowledge Distillation","metadata":{"id":"jkqr2HlzLB9a"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\ntf.random.set_seed(3)","metadata":{"id":"JVOwnv_VLB9f","execution":{"iopub.status.busy":"2024-01-01T08:06:22.070414Z","iopub.execute_input":"2024-01-01T08:06:22.071112Z","iopub.status.idle":"2024-01-01T08:06:22.085972Z","shell.execute_reply.started":"2024-01-01T08:06:22.071070Z","shell.execute_reply":"2024-01-01T08:06:22.084897Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Construct `Distiller()` class\n\nThe custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\nand `compile()`. In order to use the distiller, we need:\n\n- A trained teacher model\n- A student model to train\n- A student loss function on the difference between student predictions and ground-truth\n- A distillation loss function, along with a `temperature`, on the difference between the\nsoft student predictions and the soft teacher labels\n- An `alpha` factor to weight the student and distillation loss\n- An optimizer for the student and (optional) metrics to evaluate performance\n\nIn the `train_step` method, we perform a forward pass of both the teacher and student,\ncalculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\nand therefore we only calculate the gradients for the student weights.\n\nIn the `test_step` method, we evaluate the student model on the provided dataset.","metadata":{"id":"Zd7hAvWNLB9g"}},{"cell_type":"code","source":"\nclass Distiller(keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \"\"\" Configure the distiller.\n\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        \"\"\"\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update the metrics configured in `compile()`.\n        self.compiled_metrics.update_state(y, student_predictions)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n\n        # Calculate the loss\n        student_loss = self.student_loss_fn(y, y_prediction)\n\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_prediction)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n","metadata":{"id":"Jxg7IWuJLB9g","execution":{"iopub.status.busy":"2024-01-01T08:06:22.090372Z","iopub.execute_input":"2024-01-01T08:06:22.090918Z","iopub.status.idle":"2024-01-01T08:06:22.104521Z","shell.execute_reply.started":"2024-01-01T08:06:22.090892Z","shell.execute_reply":"2024-01-01T08:06:22.103588Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Create student and teacher models\n\nInitialy, we create a teacher model and a smaller student model. Both models are\nconvolutional neural networks and created using `Sequential()`,\nbut could be any Keras model.","metadata":{"id":"-5ABGy1mLB9h"}},{"cell_type":"code","source":"# Create the teacher\nteacher = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.Flatten(),\n        layers.Dense(10),\n    ],\n    name=\"teacher\",\n)\n\n# Create the student\nstudent = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.Flatten(),\n        layers.Dense(10),\n    ],\n    name=\"student\",\n)\n\nstudent_scratch = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Flatten(),\n        layers.Dense(10),\n    ],\n    name=\"student_scratch\",\n)\n\n# Clone student for later comparison\n#student_scratch = keras.models.clone_model(student)","metadata":{"id":"E3nCUjusLB9i","execution":{"iopub.status.busy":"2024-01-01T08:06:22.105810Z","iopub.execute_input":"2024-01-01T08:06:22.106391Z","iopub.status.idle":"2024-01-01T08:06:22.258479Z","shell.execute_reply.started":"2024-01-01T08:06:22.106357Z","shell.execute_reply":"2024-01-01T08:06:22.257690Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"teacher.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HY1UzO-XP0Y7","outputId":"c4b9bf03-b478-410b-9784-74af3ed502ac","execution":{"iopub.status.busy":"2024-01-01T08:06:22.259661Z","iopub.execute_input":"2024-01-01T08:06:22.259993Z","iopub.status.idle":"2024-01-01T08:06:22.283848Z","shell.execute_reply.started":"2024-01-01T08:06:22.259967Z","shell.execute_reply":"2024-01-01T08:06:22.282951Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"teacher\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_6 (Conv2D)           (None, 14, 14, 256)       2560      \n                                                                 \n leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 256)       0         \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 14, 14, 256)       0         \n g2D)                                                            \n                                                                 \n conv2d_7 (Conv2D)           (None, 7, 7, 512)         1180160   \n                                                                 \n flatten_3 (Flatten)         (None, 25088)             0         \n                                                                 \n dense_3 (Dense)             (None, 10)                250890    \n                                                                 \n=================================================================\nTotal params: 1433610 (5.47 MB)\nTrainable params: 1433610 (5.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"student.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_7nilDoP3vw","outputId":"df0bf7af-6abb-4bdf-a46d-83ff9a52aeb3","execution":{"iopub.status.busy":"2024-01-01T08:06:22.285163Z","iopub.execute_input":"2024-01-01T08:06:22.285882Z","iopub.status.idle":"2024-01-01T08:06:22.308538Z","shell.execute_reply.started":"2024-01-01T08:06:22.285847Z","shell.execute_reply":"2024-01-01T08:06:22.307764Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"student\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_8 (Conv2D)           (None, 14, 14, 8)         80        \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 14, 14, 8)         0         \n                                                                 \n max_pooling2d_4 (MaxPoolin  (None, 14, 14, 8)         0         \n g2D)                                                            \n                                                                 \n conv2d_9 (Conv2D)           (None, 7, 7, 8)           584       \n                                                                 \n flatten_4 (Flatten)         (None, 392)               0         \n                                                                 \n dense_4 (Dense)             (None, 10)                3930      \n                                                                 \n=================================================================\nTotal params: 4594 (17.95 KB)\nTrainable params: 4594 (17.95 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"student_scratch.summary()","metadata":{"id":"mHQBpaflxQcc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"60cc9b71-ed47-4668-bbc1-0445be63f372","execution":{"iopub.status.busy":"2024-01-01T08:06:22.309900Z","iopub.execute_input":"2024-01-01T08:06:22.310773Z","iopub.status.idle":"2024-01-01T08:06:22.328655Z","shell.execute_reply.started":"2024-01-01T08:06:22.310726Z","shell.execute_reply":"2024-01-01T08:06:22.327826Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"student_scratch\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_10 (Conv2D)          (None, 14, 14, 8)         80        \n                                                                 \n max_pooling2d_5 (MaxPoolin  (None, 14, 14, 8)         0         \n g2D)                                                            \n                                                                 \n flatten_5 (Flatten)         (None, 1568)              0         \n                                                                 \n dense_5 (Dense)             (None, 10)                15690     \n                                                                 \n=================================================================\nTotal params: 15770 (61.60 KB)\nTrainable params: 15770 (61.60 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prepare the dataset\n\nThe dataset used for training the teacher and distilling the teacher is\n[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\ndataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\nof models. Both the student and teacher are trained on the training set and evaluated on\nthe test set.","metadata":{"id":"0JyT3gmOLB9j"}},{"cell_type":"code","source":"# Prepare the train and test dataset.\nbatch_size = 128\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Normalize data\nx_train = x_train.astype(\"float32\") / 255.0\nx_train = np.reshape(x_train, (-1, 28, 28, 1))\n\nx_test = x_test.astype(\"float32\") / 255.0\nx_test = np.reshape(x_test, (-1, 28, 28, 1))\n","metadata":{"id":"_rfIdaldLB9j","outputId":"db717600-241e-4a7b-f91f-60eb2931d9a2","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-01-01T08:06:22.329993Z","iopub.execute_input":"2024-01-01T08:06:22.330650Z","iopub.status.idle":"2024-01-01T08:06:22.668388Z","shell.execute_reply.started":"2024-01-01T08:06:22.330612Z","shell.execute_reply":"2024-01-01T08:06:22.667591Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Train the teacher\n\nIn knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\nby training the teacher model on the training set in the usual way.","metadata":{"id":"T013L-f7LB9k"}},{"cell_type":"code","source":"# Train teacher as usual\nteacher.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train and evaluate teacher on data.\nteacher.fit(x_train, y_train, epochs=5)\nprint(\"---Testing Accuracy---\")\nteacher.evaluate(x_test, y_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-QiJAHCLB9k","outputId":"337994b2-683e-4642-bb3c-4fd712bd5f07","execution":{"iopub.status.busy":"2024-01-01T08:06:22.669678Z","iopub.execute_input":"2024-01-01T08:06:22.670438Z","iopub.status.idle":"2024-01-01T08:07:11.253223Z","shell.execute_reply.started":"2024-01-01T08:06:22.670396Z","shell.execute_reply":"2024-01-01T08:07:11.252285Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1420 - sparse_categorical_accuracy: 0.9570\nEpoch 2/5\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0915 - sparse_categorical_accuracy: 0.9732\nEpoch 3/5\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9761\nEpoch 4/5\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9792\nEpoch 5/5\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0711 - sparse_categorical_accuracy: 0.9804\n---Testing Accuracy---\n313/313 [==============================] - 1s 3ms/step - loss: 0.0954 - sparse_categorical_accuracy: 0.9774\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[0.09542195498943329, 0.977400004863739]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Distill teacher to student\n\nWe have already trained the teacher model, and we only need to initialize a\n`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\nhyperparameters and optimizer, and distill the teacher to the student.","metadata":{"id":"p9LH6r81LB9k"}},{"cell_type":"code","source":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=teacher)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=40,\n)\n\n# Distill teacher to student\ndistiller.fit(x_train, y_train, epochs=5)\nprint(\"---Testing Accuracy---\")\n# Evaluate student on test dataset\ndistiller.evaluate(x_test, y_test)","metadata":{"id":"4r3lsaYiLB9l","execution":{"iopub.status.busy":"2024-01-01T08:07:11.254685Z","iopub.execute_input":"2024-01-01T08:07:11.255390Z","iopub.status.idle":"2024-01-01T08:07:43.916925Z","shell.execute_reply.started":"2024-01-01T08:07:11.255352Z","shell.execute_reply":"2024-01-01T08:07:43.915958Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1875/1875 [==============================] - 7s 3ms/step - sparse_categorical_accuracy: 0.8997 - student_loss: 0.3358 - distillation_loss: 0.0174\nEpoch 2/5\n1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9565 - student_loss: 0.1584 - distillation_loss: 0.0082\nEpoch 3/5\n1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9624 - student_loss: 0.1367 - distillation_loss: 0.0067\nEpoch 4/5\n1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9671 - student_loss: 0.1204 - distillation_loss: 0.0060\nEpoch 5/5\n1875/1875 [==============================] - 6s 3ms/step - sparse_categorical_accuracy: 0.9703 - student_loss: 0.1086 - distillation_loss: 0.0054\n---Testing Accuracy---\n313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9737 - student_loss: 0.0977\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[0.9736999869346619, 0.0007057058974169195]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train student from scratch for comparison\n\nWe can also train an equivalent student model from scratch without the teacher, in order\nto evaluate the performance gain obtained by knowledge distillation.","metadata":{"id":"-KMAAbIfLB9l"}},{"cell_type":"code","source":"# Train student as done usually\nstudent_scratch.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train and evaluate student trained from scratch.\nstudent_scratch.fit(x_train, y_train, epochs=5)\nstudent_scratch.evaluate(x_test, y_test)","metadata":{"id":"-b77WIjOLB9m","execution":{"iopub.status.busy":"2024-01-01T08:07:43.918156Z","iopub.execute_input":"2024-01-01T08:07:43.918518Z","iopub.status.idle":"2024-01-01T08:08:09.021592Z","shell.execute_reply.started":"2024-01-01T08:07:43.918486Z","shell.execute_reply":"2024-01-01T08:08:09.020610Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.2879 - sparse_categorical_accuracy: 0.9177\nEpoch 2/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.1171 - sparse_categorical_accuracy: 0.9656\nEpoch 3/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0903 - sparse_categorical_accuracy: 0.9727\nEpoch 4/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0771 - sparse_categorical_accuracy: 0.9768\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9793\n313/313 [==============================] - 1s 2ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9758\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0.0761239156126976, 0.9757999777793884]"},"metadata":{}}]},{"cell_type":"markdown","source":"If the teacher is trained for 5 full epochs and the student is distilled on this teacher\nfor 3 full epochs, you should in this example experience a performance boost compared to\ntraining the same student model from scratch, and even compared to the teacher itself.\nYou should expect the teacher to have accuracy around 97.6%, the student trained from\nscratch should be around 97.6%, and the distilled student should be around 98.1%. Remove\nor try out different seeds to use different weight initializations.","metadata":{"id":"xsGB1j5BLB9m"}}]}